# rankboard


百万游戏玩家积分排行榜

# 1 背景

你正在为一个多人在线游戏开发一个全服排行榜系统，该系统需要追踪玩家获得的积分，实时更新排名，并提供排行榜实时查询功能。

请根据个人技术特长 ，编程语言不限 ，优先使用 Go。

如无不便 ，请使用 github.com，gitlab.com 或 gitee.com 提交答案， 回复公开代码库地址即可。

一、基础功能实现

请实现一个基础的排行榜系统，包含以下功能：

1.支持更新玩家积分。

2.查询玩家当前排名。

3.获取前 N 名玩家的分数和名次。

4.查询自己名次前后共 N 名玩家的分数和名次。

排名规则如下：

1.分数从高到低进行排序。

2.如果多位玩家分数相同 ，则先得到该分数的玩家排在前面。

接口定义示例

```go
interface LeaderboardService {
    // 更新玩家分数
    updateScore(playerId string, score int, timestamp int);

    // 获取玩家当前排名
    getPlayerRank(playerId string)

    // 获取排行榜前N名
    getTopN(n int) []RankInfo

    // 获取玩家周边排名
    getPlayerRankRange(playerId string, range int) []RankInfo
}
```

二、系统设计

可以使用UML图或线框图辅助表达设计

**可靠性要求**

考虑到排行榜系统需要 7*24 小时运行，且需要确保数据的一致性和完整性，请说明你会如何设计系统来满足可靠性要求。

**性能要求**

考虑到排行榜系统需要实时查询和更新，且总玩家数量可达到百万级，请说明你会如何设计系统来满足性能要求。

三、游戏需求更改（选做）

假如游戏设计师想让获得相同成绩的玩家享有同等的荣誉，所以计划调整为采用"密集排名"的计算方式，请说明你会如何修改实现来满足新的游戏需求。

新的排名规则如下：

1.分数依旧从高到低排序。

2.对于分数相同的玩家，将获得完全相同的排名位次。

3.当出现新的不同分数时，该玩家的排名将在上一个排名基础上递增 1 位。

举例说明：

```text
- 玩家A：100分 → 排名第1
- 玩家B：100分 → 排名第1
- 玩家C：95分  → 排名第2
- 玩家D：95分  → 排名第2
- 玩家E：90分  → 排名第3
```

排行榜系统需要 7*24 小时运行，且需要确保数据的一致性和完整性，如何设计系统来满足可靠性要求？
在排行榜系统需要实时查询和更新，且总玩家数量可达到百万级的情况下，如何设计系统来满足性能要求？


# 2 可靠性

redis的可靠性主要依靠以下几点:

## 2.1 RDB和AOF持久化，确保redis实例宕机后数据仍然能够恢复，不丢失数据。
AOF配置项appendfsync配置为Everysec，每秒写回：每个写命令执⾏完，只是先把⽇志写到AOF⽂件的内存缓冲区，每隔⼀秒把缓冲
区中的内容写⼊磁盘，最多只损失一秒数据。

Redis 4.0 中提出了⼀个混合使⽤ AOF ⽇志和内存快照的⽅法。简单来说，内存快照以⼀定的频率执⾏，在两次快照之间，使⽤ AOF
⽇志记录这期间的所有命令操作。这样⼀来，快照不⽤很频繁地执⾏，这就避免了频繁 fork 对主线程的影响。⽽且， AOF ⽇志也只⽤
记录两次快照间的操作，也就是说，不需要记录所有操作了，因此，就不会出现⽂件过⼤的情况了，也可以避免重写开销。

## 2.2 主从同步确保redis服务尽量少中断
Redis的做法就是增加副本冗余量，将⼀份数据同时保存在多个实例上。即使有⼀个实例出现了故障，需要过⼀段时间才能恢复，
其他实例也可以对外提供服务，不会影响业务使⽤。

repl_backlog_buffer是⼀个环形缓冲区，主库会记录⾃⼰写到的位置，从库则会记录⾃⼰已经读到的位置。
要重点监控master_repl_offset和slave_repl_offset是否一致，据此判断主从同步是否存在延迟。

## 2.3 哨兵机制自动实现故障转移
设置奇数个哨兵(至少3个)组成哨兵集群监控主从库，一旦有哨兵实例判定主库主观下线，且得到仲裁所需的赞成票数后，就可以标记
主库为“客观下线”，哨兵集群经过leader选举出一个哨兵实例执行主从切换，然后根据slave-priority(从库优先级)配置项，
同步进度等从所有从库中选出最合适的从库作为新主库。

要保证所有哨兵实例的配置是⼀致的，尤其是主观下线的判断值downafter-milliseconds。如果这个值在不同的哨兵实例
上配置不⼀致，就会导致哨兵集群⼀直没有对有故障的主库形成共识，也就无法及时切换主库，最终的结果就是集群服务不稳定。

## 2.4 合理配置min-slaves-to-write和min-slaves-max-lag配置项防止脑裂
min-slaves-to-write：这个配置项设置了主库能进⾏数据同步的最少从库数量；
min-slaves-max-lag：这个配置项设置了主从库间进⾏数据复制时，从库给主库发送ACK消息的最⼤延迟
（以秒为单位）

我们可以把min-slaves-to-write和min-slaves-max-lag这两个配置项搭配起来使⽤，分别给它们设置⼀定的
阈值，假设为N和T。这两个配置项组合后的要求是，主库连接的从库中⾄少有N个从库，和主库进⾏数据复
制时的ACK消息延迟不能超过T秒，否则，主库就不会再接收客⼾端的请求了。

假设我们将min-slaves-to-write设置为1，把min-slaves-max-lag设置为12s，把哨兵的down-aftermilliseconds设置为10s，
主库因为某些原因卡住了15s，导致哨兵判断主库客观下线，开始进⾏主从切换。 同时，因为原主库卡住了15s，没有⼀个从库能和原主库
在12s内进⾏数据复制，原主库也⽆法接收客⼾端请求了。这样⼀来，主从切换完成后，也只有新主库能接收请求，不会发⽣脑裂，也就不会
发⽣数据丢失的问题了。



# 3 高性能

## 3.1 使用协程池来复用 goroutine，减轻 runtime 的调度压力以及内存压力
使用开源的高性能协程池库ants, 大幅减少了频繁创建和销毁协程的开销，以及内存占用和协程调度，切换的开销，
在大规模 goroutine 并发的场景下可以极大地提高并发性能。试想一下:如果不适用协程池，那么极端情况下每秒100万
个用户都来查询自己的排名，而每个查询排名的请求对应12个协程去各个分片中查询(zset集合)的局部排名，
最后在主协程中进行汇总累加，那么总共就是(12+1)x100w=1300w个协程，如果部署13个节点组成服务器集群来进行负载均衡，
那么每个服务节点每秒并发的协程数也高达约100万，此等水平的协程暴涨很有可能使得服务器的CPU负载和内存占用飙升，
导致服务器被压垮，这绝对是不可承受之重。

优化后的协程数计算
在优化后的系统中，每个节点的总协程数由两部分组成：
主协程数：取决于同时处理的请求数（并发查询数）。根据前面的假设，每秒10,0000 QPS，处理时间 10ms，则并发查询数
10万x0.01 ≈ 1000 个。 因此，主协程数约为 1000 个。
工作协程数：由协程池控制，固定为 1000 个 worker。这 1000 个 worker 被所有查询任务共享，无论有多少个并发查询，
工作协程数始终是 10。 因此，总协程数 = 主协程数 + 工作协程数 = 1000 + 1000 = 2000 个。
2000个协程处理起来还是游刃有余的。


## 3.2 使用本地缓存避免通过网络IO请求Redis以及全局排名的聚合计算开销
如果业务方能接受短时间内的数据不一致，我们可以使用开源的第三方本地缓存库bigcache，将计算开销大的玩家全局排名缓存在本地内存，
这样既避免了频繁的网络IO请求， 也节省了聚合计算开销，可大幅降低接口响应时间(RT)，本地缓存设置10分钟的有效期。

## 3.3 利用redis的高性能支撑高并发
磁盘型数据库MySQL只能支撑数千QPS，而单机redis实例就足以支撑10万QPS，极端情况下百万玩家同时更新积分或查看排名，
那就是百万QPS，单机单机redis实例肯定扛不住，必须使用多个分片，比如我们使用12个实例组成分片集群，足以支撑120万QPS，
还略有冗余。

## 3.4 控制单实例的数据量，避免bgsave⼦进程进⾏RDB持久化时阻塞主线程，关闭内存大页机制
redis单实例的数据年应控制在2-4GB，如果数据量过大，bgsave⼦进程进⾏RDB持久化时阻塞主线程。
虽然是fork子进程做RDB持久化，子进程虽然不会阻塞主线程，但是fork子进程操作本身可能会阻塞主线程，数据量越大，
意味着需要拷贝的内存页表越大，越容易阻塞主线程。

⼦进程进⾏RDB持久化采用的是写时复制(copy on write)技术，如果主线程要修改⼀块数据那么，这块数据就会被复制⼀份，⽣成
该数据的副本。然后， bgsave ⼦进程会把这个副本数据写⼊ RDB ⽂件，⽽在这个过程中，主线程仍然可以直接修改原来的数据。
如果采⽤了内存⼤⻚，那么，即使客⼾端请求只修改100B的数据，Redis也需要拷⻉2MB的⼤⻚。相反，如果是常规内存⻚机制，只⽤
拷⻉4KB。两者相⽐，你可以看到，当客⼾端请求修改或新写⼊数据较多时，内存⼤⻚机制将导致⼤量的拷⻉，这就会影响Redis正常
的访存操作，最终导致性能变慢。

## 3.5 给Redis实例绑定物理核，降低请求响应延迟

如果在CPU多核场景下，Redis实例被频繁调度到不同CPU核上运⾏的话，那么，对Redis实例的请求处理时
间影响就很⼤了。每调度⼀次，⼀些请求就会受到运⾏时信息、指令和数据重新加载过程的影响，这就会导
致某些请求的延迟明显⾼于其他请求。分析到这⾥，我们就知道了刚刚的例⼦中99%尾延迟的值始终降不下
来的原因。

所以，我们要避免Redis总是在不同CPU核上来回调度执⾏。于是，我们尝试着把Redis实例和CPU核绑定
了，让⼀个Redis实例固定运⾏在⼀个CPU核上。我们可以使⽤taskset命令把⼀个程序绑定在⼀个核上运⾏。

⽐如说，我们执⾏下⾯的命令，就把Redis实例绑在了0号和12号逻辑核(同一个物理核)上，其中，“-c”选项⽤于设置要绑定的核编号。

```shell
taskset -c 0,12 ./redis-server
```

## 3.6 NUMA架构下，注意把Redis实例和⽹络中断处理程序运⾏在同⼀个CPU Socket上

如果⽹络中断处理程序和Redis实例各⾃所绑的CPU核不在同⼀个CPU Socket上，那么，Redis实例
读取⽹络数据时，就需要跨CPU Socket访问内存，这个过程会花费较多时间。

所以，为了避免Redis跨CPU Socket访问⽹络数据，我们最好把⽹络中断程序和Redis实例绑在同⼀个CPU
Socket上，这样⼀来，Redis实例就可以直接从本地内存读取⽹络数据了。

## 3.7 避免出现big key，将big key分而治之，以免操作big key阻塞主线程
本项目代码就是将每个分片的big key划分为多个不同的小集合，使得单键对应的集合元素降低到1万以下。
